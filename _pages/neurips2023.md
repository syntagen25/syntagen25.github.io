---
layout: home
order: 1
permalink: /
title: About
desc_title: SyntaGen - Harnessing Generative Models for Synthetic Visual Datasets
social: true
---
Computer vision has been rapidly transformed by advancements in generative models, particularly in text-to-image generation with models like Imagen 3, Stable Diffusion 3, Flux, and DALLE-3, as well as text-to-video models such as Sora, Stable Video Diffusion, and Meta MovieGen. In the realm of 3D generation, models like Zero-123, Instant 3D, and the Large Reconstruction Model (LRM) have pushed the boundaries of 3D content creation. These innovations have enabled the development of highly realistic and diverse synthetic visual datasets, complete with annotations and rich variations, which are invaluable for training and evaluating algorithms in object detection, segmentation, representation learning, and scene understanding. The second SyntaGen Workshop aims to foster collaboration and knowledge exchange across the field, bringing together experts and practitioners to propel the development of generative models and synthetic visual datasets to new heights. Through talks, paper presentations, poster sessions, and panel discussions, the workshop will catalyze breakthroughs at the intersection of generative models and computer vision applications.

<div class="content">
  <!-- <p>The field of computer vision has undergone a significant transformation in recent years with the advancement of generative models...</p> -->
  
  <!-- <div class="cta">
      <span class="gift-icon" id="gift-icon">
          <i class="fa fa-gift" aria-hidden="true"></i>
          Attend the SyntaGen Workshop and get our exclusive gifts!
          <i class="fa fa-gift" aria-hidden="true"></i>
      </span>
      <div class="dropdown-content" id="dropdown-content">
          <div class="gift-images">
              <img src="assets/img2/totebag2.png" alt="Gift Tote Bag" class="gift-image">
              <img src="assets/img2/Cap2.png" alt="Gift Cap" class="gift-image">
          </div>
      </div>
  </div> -->

</div>

### **Speakers**

<table style="width:100%">
  <tr>
    <td style="text-align:center border-radius:50%">
      <a href="https://varunjampani.github.io/"><img src="assets/img2/varun2.jpeg" style="border-radius:50%" height="150" width="150"></a>
    </td>
    <td style="text-align:center">
      <a href="https://research.adobe.com/person/nathan-carr/"><img src="assets/img2/nathan.png" height="150" width="150" style="border-radius:50%;"></a>
    </td>
    <td style="text-align:center">
      <a href="https://jbhuang0604.github.io/"><img src="assets/img2/jinbin2.jpg" height="150" width="150" style="border-radius:50%;"></a>
    </td>
    <td style="text-align:center">
      <a href="https://stulyakov.com/"><img src="assets/img2/sergrey.jpg" height="150" width="150" style="border-radius:50%;"></a>
    </td>
    <td style="text-align:center">
      <a href="https://ssundaram21.github.io/"><img src="assets/img2/shobhita.png" height="150" width="150" style="border-radius:50%;"></a>
    </td>
  </tr>
  <tr>
    <td style="text-align:center"><a href="https://varunjampani.github.io/">Varun Jampani</a><br>VP Research<br>Stability AI</td>
    <td style="text-align:center"><a href="https://research.adobe.com/person/nathan-carr/">Nathan Carr</a><br>Adobe Fellow<br>Adobe Research</td>
    <td style="text-align:center"><a href="https://jbhuang0604.github.io/">Jia-Bin Huang</a><br>Associate Professor<br>University of Maryland College Park</td>
    <td style="text-align:center"><a href="https://stulyakov.com/">Sergey Tulyakov</a><br>Director of Research<br>Snap Inc.</td>
    <td style="text-align:center"><a href="https://ssundaram21.github.io/">Shobhita Sundaram</a><br>MIT</td>
    
  </tr>
</table>

### **Schedule**

<table>
  <thead>
    <tr>
      <th style="text-align:left;">Time</th>
      <th>Event</th>
      <th style="text-align:left;">Duration</th>
      <th style="text-align:left;">Speaker</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align:left;">8:30</td>
      <td>Introduction</td>
      <td style="text-align:left;">10 mins</td>
      <td style="text-align:left;"></td>
    </tr>
    <tr>
      <td style="text-align:left;">8:35</td>
      <td>Oral presentation</td>
      <td style="text-align:left;">25 mins</td>
      <td style="text-align:left;"></td>
    </tr>
    <tr>
      <td style="text-align:left;">9:00</td>
      <td><b>Invited talk 1: </b> Inventing Data: An Industry Perspective</td>
      <td style="text-align:left;">25 mins</td>
      <td style="text-align:left;"><b>Nathan Carr</b></td>
    </tr>
    <tr>
      <td style="text-align:left;">9:25</td>
      <td><b>Invited talk 2:</b> Beyond 3D: Generating Volumetric Scenes with Motion</td>
      <td style="text-align:left;">25 mins</td>
      <td style="text-align:left;"><b>Sergey Tulyakov</b></td>
    </tr>
    <tr>
      <td style="text-align:left;">9:50</td>
      <td>Break</td>
      <td style="text-align:left;">10 mins</td>
      <td style="text-align:left;"></td>
    </tr>
    <tr>
      <td style="text-align:left;">10:00</td>
      <td><b>Invited talk 3:</b> Personalized Representation from Personalized Generation</td>
      <td style="text-align:left;">25 mins</td>
      <td style="text-align:left;"><b>Shobhita Sundaram</b></td>
    </tr>
    <tr>
      <td style="text-align:left;">10:25</td>
      <td><b>Invited talk 4: </b>  Learning to Recreate Reality</td>
      <td style="text-align:left;">25 mins</td>
      <td style="text-align:left;"><b>Jia-Bin Huang</b></td>
    </tr>
    <tr>
      <td style="text-align:left;">10:50</td>
      <td><b>Invited talk 5:</b> Diffusion Dialed in: Light and Heavy Adaptation of Diffusion Models for Complex Vision Tasks</td>
      <td style="text-align:left;">25 mins</td>
      <td style="text-align:left;"><b>Varun Jampani</b></td>
    </tr>
    <tr>
      <td style="text-align:left;">11:15</td>
      <td>Panel discussion</td>
      <td style="text-align:left;">25 mins</td>
      <td style="text-align:left;">TBD</td>
    </tr>
    <tr>
      <td style="text-align:left;">11:40</td>
      <td>Poster Session</td>
      <td style="text-align:left;">60 mins</td>
      <td style="text-align:left;"></td>
    </tr>
  </tbody>
</table>

### **Accepted Papers**
**[Oral]** [NeIn: Telling What You Don’t Want](https://openreview.net/forum?id=6oNlkmuxYg). Nhat-Tan Bui, Dinh-Hieu Hoang, Quoc-Huy Trinh, Minh-Triet Tran, Truong Nguyen, Susan Gauch \
**[Oral]** [Latent Video Dataset Distillation](https://openreview.net/forum?id=i665TIHv92). Ning Li, Antai Andy Liu, Jingran Zhang, Justin Cui \
**[Oral]** [Eye Tell the Truth: GazeVal Highlights Shortcomings of Generative AI in Medical Imaging](https://openreview.net/forum?id=m0qWnLwRNm). David Wong, Bin Wang, Gorkem Durak, Marouane Tliba, Akshay S Chaudhari, Aladine Chetouani, Ahmet Enis Cetin, Cagdas Topel, Nicolo Gennaro, Camila Lopes Vendrami, Tugce Agirlar Trabzonlu, Amir Ali Rahsepar, Laetitia Perronne, Matthew Antalek, Onural Ozturk, Gokcan Okur, Andrew C Gordon, Ayis Pyrros, Frank H Miller, Amir A. Borhani, Hatice Savas, Eric Hart, Drew Torigian, Jayaram K Udupa, Elizabeth Anne Krupinski, Ulas Bagci  \
**[Poster]** [Bridging 3D Editing and Geometry-Consistent Paired Dataset Creation for 2D Nighttime-to-Daytime Translation](https://openreview.net/forum?id=7UXOSWZ0C8). Xiao Cao, Yuyang Zhao, Robby T. Tan, Zhiyong Huang \
**[Poster]** [VideoHandles: Editing 3D Object Compositions in Videos Using Video Generative Priors](https://openreview.net/forum?id=HqswBG16Pi). Juil Koo, Paul Guerrero, Chun-Hao Paul Huang, Duygu Ceylan, Minhyuk Sung \
**[Poster]** [Noise Consistency Regularization for Improved Subject-Driven Image Synthesis](https://openreview.net/forum?id=5LvxYOSPku). Yao Ni, Song Wen, Piotr Koniusz, Anoop Cherian \
**[Poster]** [AnomalyHybrid: A Domain-agnostic Generative Framework for General Anomaly Detection](https://openreview.net/forum?id=Y8wEJRCFrF). Ying Zhao \
**[Poster]** [SVAD: From Single Image to 3D Avatar via Synthetic Data Generation with Video Diffusion and Data Augmentation](https://openreview.net/forum?id=S4SrbuOEcz). Yonwoo Choi \
**[Poster]** [good4cir: Generating Detailed Synthetic Captions for Composed Image Retrieval](https://openreview.net/forum?id=8ckgIZeKCa). Pranavi Kolouju, Eric Xing, Robert Pless, Nathan Jacobs, Abby Stylianou \
**[Poster]** [Syn3DTxt: Embedding 3D Cues for Scene Text Generation](https://openreview.net/forum?id=QmY75NG5Vp). Li-Syun Hsiung, Jun Kai Tu, Kuan-wu Chu, Yu-Hsuan Chiu, Yan-Tsung Peng, Sheng-Luen Chung, Gee-Sern Jison Hsu 


### **Call for Papers**
We invite papers to propel the development of generative models and/or the use of their synthetic visual datasets for training and evaluating computer vision models. Accepted papers will be presented in the poster session in our workshop. We welcome submissions along two tracks:
* Full papers: Up to 8 pages, excluding references, with option for inclusion in the proceedings.
* Short papers: Up to 4 pages, excluding references, not for the proceedings.

Only full papers will be considered for the Best Paper award. Additionally, we offer a Best Paper and a Best Paper Runner-up award with oral presentations. All accepted papers without inclusion in the proceedings are non-archival.
#### **Topics**
The main objective of the SyntaGen workshop is to offer a space for researchers, practitioners, and enthusiasts to investigate, converse, and cooperate on the development, use, and potential uses of synthetic visual datasets made from generative models. The workshop will cover various topics, including but not restricted to:
* Leveraging pre-trained generative models to generate data and annotations for perception-driven tasks, including image classification, representation learning, object detection, semantic and instance segmentation, relationship detection, action recognition, object tracking, and 3D shape reconstruction and recognition.
* Extending the generative capacity of large-scale pre-trained text-to-image models to other domains, such as videos, 3D, and 4D spaces.
* Exploring new research directions in generative models, including GANs, VAEs, diffusion models, and autoregressive models, to advance visual content generation.
* Synergizing expansive synthetic datasets with minimally annotated real datasets to enhance model performance across scenarios including unsupervised, semi-supervised, weakly-supervised, and zero-shot/few-shot learning.
* Enhancing data quality and improving synthesis methodologies in the context of pre-trained text-to-image (T2I), text-to-video (T2V), text-to-3D, and text-to-4D models.
* Evaluating the quality and effectiveness of the generated datasets, particularly on metrics, challenges, and open problems related to benchmarking synthetic visual datasets.
* Ethical implications of using synthetic annotated data, strategies for mitigating biases, verifying and protecting generated visual contents, and ensuring responsible data generation and annotation practices.

#### **Submisison Instructions**
Submissions should be anonymized and formatted using the [CVPR 2025 template](https://github.com/cvpr-org/author-kit/releases) and uploaded as a single PDF.

<p style="color:red;"><b>Notes for registering a new OpenReview account</b></p>
* New profiles created without an institutional email will go through a moderation process that can take up to two weeks.
* New profiles created with an institutional email will be activated automatically.

#### **Supplemental Material**
Supplemental materials optionally can be submitted along the paper manuscript on the submission deadline. They must be anonymized and uploaded either as a single PDF or a ZIP file.

#### **Submission Link**

[Openreview submission link](https://openreview.net/group?id=thecvf.com/CVPR/2025/Workshop/SyntaGen#tab-your-consoles)

#### **Important workshop dates**

<!-- * TBD -->

* Submission deadline: **March 22nd, 11:59 PM PST**
* Review and Decision release: ~~April 3rd, 11:59 PM PST~~ **March 30th, 11:59 PM PST**
* Metadata Submission for included in CVPR workshop’s proceedings **(new)**: **March 31st, 11:59 PM PST**
* Camera Ready: ~~April 7th, 11:59 PM PST (Included in Proceedings) or April 14th, 11:59 PM PST (Not included in Proceedings)~~ **April 14th, 11:59 PM PST** for included and non-included in CVPR workshop's proceedings.
* Workshop date: **Jun 12th**

<!-- Mar 30: Review and decision release
Mar 31 (new): Metadata Submission for included in CVPR workshop’s proceedings
April 14th: Camera ready for both options: included and non-included in CVPR workshop’s proceedings -->

### **Workshop Sponsors**
<table style="width:100%; align: left; border: none; spacing: none">
  <tr style="border: none; spacing: none">
    <td style="text-align:center; border: none; spacing: none"><a href="https://research.adobe.com/"><img src="assets/img2/aff-adobe.jpg" height="70"></a></td>
  </tr>
  <tr style="border: none; spacing: none"> 
    <td style="text-align:center; border: none; spacing: none"><a href="https://www.adobe.com/"><b>Adobe</b></a></td>
  </tr>
</table>

### **Organizers**

<table style="width:100%">
  <tr>
    <td style="text-align:center border-radius:50%">
      <a href="https://khoinguyen.org"><img src="assets/img2/org-khoinguyen.jpg" style="border-radius:50%;" height="150" width="150"></a>
    </td>
    <td style="text-align:center">
      <a href="https://scholar.google.com/citations?user=FYZ5ODQAAAAJ&hl=en"><img src="assets/img/org-anh-tran-square.jpg" style="border-radius:50%;" height="150" width="150"></a>
    </td>
    <td style="text-align:center">
      <a href="https://sonhua.github.io/"><img src="assets/img2/org-sonhua.jpg" height="150" width="150" style="border-radius:50%;"></a>
    </td>
    <td style="text-align:center">
      <a href="https://www.supasorn.com/"><img src="assets/img2/org-supasorn.jpg" height="150" width="150" style="border-radius:50%;"></a>
    </td>
    <td style="text-align:center">
      <a href="https://zhouyisjtu.github.io/"><img src="assets/img2/org-yizhou.png" height="150" width="150" style="border-radius:50%;"></a>
    </td>
  </tr>
  <tr>
    <td style="text-align:center"><a href="https://khoinguyen.org">Khoi Nguyen</a> <br>Qualcomm AI Research, Vietnam</td>
    <td style="text-align:center"><a href="https://scholar.google.com/citations?user=FYZ5ODQAAAAJ&hl=en">Anh Tuan Tran</a> <br>Qualcomm AI Research, Vietnam</td>
    <td style="text-align:center"><a href="https://sonhua.github.io/">Binh Son Hua</a><br>Trinity College Dublin, Ireland</td>
    <td style="text-align:center"><a href="https://www.supasorn.com/">Supasorn Suwajanakorn</a> <br>VISTEC, Thailand</td>
    <td style="text-align:center"><a href="https://zhouyisjtu.github.io/">Yi Zhou</a><br>Adobe</td>
  </tr>
</table>


<!-- ### **Volunteers** -->

<!-- <table style="width:100%">
  <tr>
    <td style="text-align:center">
      <a href="https://truongvu2000nd.github.io/"><img src="assets/img2/truong.jpg" height="150" width="150" style="border-radius:50%;"></a>
    </td>
    <td style="text-align:center">
      <a href="https://quang-ngh.github.io/"><img src="assets/img2/quang.jpg" height="150" width="150" style="border-radius:50%;"></a>
    </td>
  </tr>
  <tr>
    <td style="text-align:center"><a href="https://truongvu2000nd.github.io/">Truong Vu</a><br>Research Resident<br>VinAI Research, Vietnam</td>
    <td style="text-align:center"><a href="https://quang-ngh.github.io/">Quang Nguyen</a><br>Research Resident<br>VinAI Research, Vietnam</td>
  </tr>
</table> -->


### **Organizers affiliations**

<!-- <td style="text-align:center"><a href="https://vinuni.edu.vn/college-of-engineering-computer-science/"><img src="assets/img/inst-vinuni-cecs.png" height="75"></a></td>

<td style="text-align:center"><a href="https://www.cs.umd.edu/"><img src="assets/img/inst-umd-cs.png" height="75"></a></td>
<br>

<td style="text-align:center"><a href="https://www.vinai.io/"><img src="assets/img/inst-vinai.png" height="75"></a></td>
<br>


<td style="text-align:center"><a href="https://www.clemson.edu/index.html"><img src="assets/img/inst-clemson.png" height="75"></a></td>
<br>

<td style="text-align:center"><a href="https://www.deakin.edu.au/"><img src="assets/img/inst-deakin.png" height="75"></a></td>
<br>

<td style="text-align:center"><a href="https://tech.cornell.edu/"><img src="assets/img/inst-cornell-tech.png" height="75"></a></td>
<br>

<td style="text-align:center"><a href="https://www.nyu.edu/"><img src="assets/img/New_York_University-Logo.png" height="75"></a></td> -->

<table style="width:100%; align: left; border: none; spacing: none">
  <tr style="border: none; spacing: none"> 
    <td style="text-align:center; border: none; spacing: none"><a href="https://www.qualcomm.com/research/artificial-intelligence"><img src="assets/img2/Q_logo.png" height="100"></a></td>  
    <td style="text-align:center; border: none; spacing: none"><a href="https://www.tcd.ie/"><img src="assets/img2/aff-tcd.png" height="80"></a></td>
    <td style="text-align:center; border: none; spacing: none"><a href="https://www.vistec.ac.th/"><img src="assets/img2/aff-vistec.jpeg" height="70"></a></td>
    <td style="text-align:center; border: none; spacing: none"><a href="https://research.adobe.com/"><img src="assets/img2/aff-adobe.jpg" height="70"></a></td>
  </tr>
</table> 
