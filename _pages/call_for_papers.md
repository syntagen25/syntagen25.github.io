---
layout: call_for_papers
order: 3
permalink: /papers
title: Call for Papers
desc_title: Call for Papers
social: true
---

We invite papers to propel the development of generative models and/or the use of their synthetic visual datasets for training and evaluating computer vision models. Accepted papers will be presented in the poster session in our workshop. Additionally, we offer a Best Paper and a Best Paper Runner-up award with oral presentations.

### Topics
The main objective of the SyntaGen workshop is to offer a space for researchers, practitioners, and enthusiasts to investigate, converse, and cooperate on the development, use, and potential uses of synthetic visual datasets made from generative models. The workshop will cover various topics, including but not restricted to:
* Leveraging pre-trained generative models to generate data and annotations for perception-driven tasks, including image classification, object detection, semantic and instance segmentation, relationship detection, action recognition, object tracking, and 3D shape reconstruction and recognition.
* Extending the generative capacity of large-scale pre-trained text-to-image models to other domains, such as videos and 3D spaces.
* Synergizing expansive synthetic datasets with minimally annotated real datasets to enhance model performance across scenarios including unsupervised, semi-supervised, weakly-supervised, and zero-shot/few-shot learning.
* Exploring generative model learning from small-scale datasets, paving the way for effective data generation when faced with limited training data.
* Enhancing data quality and improving synthesis methodologies in the context of pre-trained text-to-image (T2I), text-to-video (T2V), and text-to-3D models.
* Evaluating the quality and effectiveness of the generated datasets, particularly on metrics, challenges, and open problems related to benchmarking synthetic visual datasets.
* Ethical implications of using synthetic annotated data, strategies for mitigating biases, and ensuring responsible data generation and annotation practices.

### Submission Instructions
We invite contributions in the form of full papers (up to 8 pages, excluding references, for inclusion in the proceedings) or short papers (up to 4 pages, excluding references, not for the proceedings). Only full papers will be considered for the Best Paper award. Submissions should be anonymized and formatted using the [CVPR 2024 template](https://github.com/cvpr-org/author-kit/releases) and uploaded as a single PDF. Note that our workshop is non-archival.

#### Supplementary material
Supplemental materials optionally can be submitted along the paper manuscript on the submission deadline. They must be anonymized and uploaded either as a single PDF or a ZIP file.

#### Submission link
[Link](https://openreview.net/group?id=thecvf.com/CVPR/2024/Workshop/SyntaGen )

### Important workshop dates
* **Submission deadline:** March 22nd, 11:59 PM Pacific Time
* **Notification of acceptance:** April 7th, 11:59 PM Pacific Time
* **Camera Ready submission deadline:** April 14th, 11:59 PM Pacific Time
* **Workshop date:** June 17th, 2024 (Morning)

<!-- We cordially invite submissions and participation in our “Backdoors in Deep Learning: The Good, the Bad, and the Ugly” workshop (neurips2023-bugs.github.io) that will be held on December 15 or 16, 2023 at NeurIPS 2023, New Orleans, USA.  -->

<!-- The submission deadline is **<s>September 29, 2023</s> October 6th, 2023, 23:59 AoE** and the submission link <a href="https://openreview.net/group?id=NeurIPS.cc/2023/Workshop/BUGS">https://openreview.net/group?id=NeurIPS.cc/2023/Workshop/BUGS</a>.

#### Motivation and Topics

The main objective of the SyntaGen workshop is to offer a space for researchers, practitioners, and
enthusiasts to investigate, converse, and cooperate on the development, use, and potential uses of
synthetic visual datasets made from generative models. The workshop will cover various topics,
including but not restricted to:

* Leveraging pre-trained generative models to generate data and annotations for perception-driven tasks, including image classification, object detection, semantic and instance segmentation, relationship detection, action recognition, object tracking, and 3D shape reconstruction and recognition.
* Extending the generative capacity of large-scale pre-trained text-to-image models to other
domains, such as videos and 3D spaces.
* Synergizing expansive synthetic datasets with minimally annotated real datasets to enhance
model performance across scenarios including unsupervised, semi-supervised, weakly supervised, and zero-shot/few-shot learning.
* Enhancing data quality and improving synthesis methodologies in the context of pre-trained
text-to-image (T2I), text-to-video (T2V), and text-to-3D models.
* Evaluating the quality and effectiveness of the generated datasets, particularly on metrics,
challenges, and open problems related to benchmarking synthetic visual datasets.
* Ethical implications of using synthetic annotated data, strategies for mitigating biases, and
ensuring responsible data generation and annotation practices.

We only consider submissions that haven’t been published in any peer-reviewed venue, including NeurIPS 2023 conference. **We allow dual submissions with other workshops or conferences. The workshop is non-archival and will not have any official proceedings**. All accepted papers will be allocated either a poster presentation or a talk slot.
 

### Submission Instructions

Papers should be submitted to OpenReview: <a href="https://openreview.net/group?id=NeurIPS.cc/2023/Workshop/BUGS">https://openreview.net/group?id=NeurIPS.cc/2023/Workshop/BUGS</a>

Submitted papers should have up to 6 pages (excluding references, acknowledgments, or appendices). Please use the NeurIPS submission template provided at <a href="https://neurips.cc/Conferences/2023/PaperInformation/StyleFiles">https://neurips.cc/Conferences/2023/PaperInformation/StyleFiles</a>.
Submissions must be anonymous following NeurIPS double-blind reviewing guidelines, NeurIPS Code of Conduct, and Code of Ethics. Accepted papers will be hosted on the workshop website but are considered non-archival and can be submitted to other workshops, conferences, or journals if their submission policy allows. -->